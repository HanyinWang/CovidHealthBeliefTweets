{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_annotation = pd.read_csv('./data/annotation_final.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 4515, 1: 485})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(all_annotation['Perceived_susceptibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HBM related Counter({1: 2739, 0: 2261}) \n",
      "Perceived barriers Counter({0: 3838, 1: 1162}) \n",
      "Perceived benefits Counter({0: 3899, 1: 1101}) \n",
      "Perceived severity Counter({0: 4512, 1: 488}) \n",
      "Perceived susceptibility Counter({0: 4515, 1: 485})\n"
     ]
    }
   ],
   "source": [
    "print('HBM related', Counter(all_annotation['HBM_related']), '\\n'\n",
    "      'Perceived barriers', Counter(all_annotation['Perceived_barriers']), '\\n'\n",
    "     'Perceived benefits', Counter(all_annotation['Perceived_benefits']), '\\n'\n",
    "     'Perceived severity', Counter(all_annotation['Perceived_severity']), '\\n'\n",
    "     'Perceived susceptibility', Counter(all_annotation['Perceived_susceptibility']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_annotation['read_text_clean2'], all_annotation['HBM_related'],\n",
    "                                                   test_size = 0.2, random_state = 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 183), (1000, 183))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(stop_words = set(stopwords.words('english')), min_df = 0.01, max_df = 0.99, token_pattern = '(?u)\\\\b[A-Za-z][A-Za-z]+\\\\b')\n",
    "X_train_count = count_vect.fit_transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)\n",
    "\n",
    "X_train_count.shape, X_test_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_name = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_count)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_final = X_train_tfidf.toarray()\n",
    "X_test_final = X_test_tfidf.toarray()\n",
    "\n",
    "y_train_final = y_train.values\n",
    "y_test_final = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_param(clf, name, param_test):\n",
    "    if param_test:\n",
    "        gsearch = GridSearchCV(estimator = clf, param_grid = param_test, n_jobs = -1, cv = 5)\n",
    "        gsearch.fit(X_train_final, y_train_final)\n",
    "        best_param = gsearch.best_params_\n",
    "\n",
    "        return best_param\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(clf,name):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    \n",
    "    if name !='Ridge_Classifier' and name != 'LinearSVC_L1' and name != 'LinearSVC_L2' and name != 'Perceptron' and name != 'Passive_Aggressive' and name != 'SGDClassifier_L1' and name != 'SGDClassifier_L2' and name != 'Elastic_Net_penalty':\n",
    "        clf.fit(X_train_final, y_train_final)\n",
    "        predictions = clf.predict(X_train_final)\n",
    "        proba = clf.predict_proba(X_train_final)\n",
    "        pred = (clf.predict_proba(X_train_final)[:,1] >= 0.45).astype(bool)\n",
    "    else:\n",
    "        clf2 = CalibratedClassifierCV(clf)\n",
    "        clf2.fit(X_train_final, y_train_final)\n",
    "        predictions = clf2.predict(X_train_final)\n",
    "        proba = clf2.predict_proba(X_train_final)\n",
    "        pred = (clf2.predict_proba(X_train_final)[:,1] >= 0.45).astype(bool)\n",
    "        \n",
    "    # find threshold\n",
    "    threshold = Find_Optimal_Cutoff(y_train_final, proba[:,1])\n",
    "    pred = (proba[:,1] >= threshold).astype(bool)\n",
    "        \n",
    "    accuracy = metrics.accuracy_score(y_train_final, predictions)\n",
    "    aucscore = metrics.roc_auc_score(y_train_final, proba[:,1])\n",
    "    \n",
    "    precision_binary = metrics.precision_score(y_train_final, predictions, average='binary') \n",
    "    recall_binary = metrics.recall_score(y_train_final, predictions,average='binary') \n",
    "    fmeasure_binary = metrics.f1_score(y_train_final, predictions,average='binary')\n",
    "    f1_micro = metrics.f1_score(y_train_final, predictions,average='micro')\n",
    "    f1_macro = metrics.f1_score(y_train_final, predictions,average='macro')\n",
    "    \n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_train_final, predictions).ravel()\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    youden_index = sensitivity+specificity-1\n",
    "    \n",
    "    print()\n",
    "    # clf_descr, score.mean(), score.std(), train_time, test_time\n",
    "    return name, accuracy, aucscore, precision_binary, recall_binary, fmeasure_binary, f1_micro, f1_macro, sensitivity, specificity, youden_index, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def heldout(clf,name,thre):\n",
    "    print('_' * 80)\n",
    "    print(\"Test: \")\n",
    "    print(clf)\n",
    "    \n",
    "    if name != 'Ridge_Classifier' and name !='LinearSVC_L1' and name != 'LinearSVC_L2' and name != 'Perceptron' and name != 'Passive_Aggressive' and name != 'SGDClassifier_L1' and name != 'SGDClassifier_L2' and name != 'Elastic_Net_penalty':\n",
    "        clf.fit(X_train_final, y_train_final)\n",
    "        predictions = clf.predict(X_test_final)\n",
    "        proba = clf.predict_proba(X_test_final)\n",
    "    else:\n",
    "        clf2 = CalibratedClassifierCV(clf)\n",
    "        clf2.fit(X_train_final, y_train_final)\n",
    "        predictions = clf2.predict(X_test_final)\n",
    "        proba = clf2.predict_proba(X_test_final)\n",
    "    \n",
    "    # find threshold\n",
    "    pred = (proba[:,1] >= thre).astype(bool)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_test_final, predictions)\n",
    "    aucscore = metrics.roc_auc_score(y_test_final, proba[:,1])\n",
    "    \n",
    "    precision_binary = metrics.precision_score(y_test_final, predictions, average='binary') \n",
    "    recall_binary = metrics.recall_score(y_test_final, predictions,average='binary') \n",
    "    fmeasure_binary = metrics.f1_score(y_test_final, predictions,average='binary')\n",
    "    f1_micro = metrics.f1_score(y_test_final, predictions,average='micro')\n",
    "    f1_macro = metrics.f1_score(y_test_final, predictions,average='macro')\n",
    "    \n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test_final, predictions).ravel()\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    youden_index = sensitivity+specificity-1\n",
    "    \n",
    "    print()\n",
    "    print(metrics.classification_report(y_test_final, predictions, digits=4))\n",
    "    # clf_descr, score.mean(), score.std(), train_time, test_time\n",
    "    return name, accuracy, aucscore, precision_binary, recall_binary, fmeasure_binary, f1_micro, f1_macro, sensitivity, specificity, youden_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params = []\n",
    "for clf, name, param_test in (\n",
    "        (RidgeClassifier(tol=1e-2, solver = 'sag', max_iter = 1000, class_weight = 'balanced', random_state = 99), \"Ridge_Classifier\", None),\n",
    "        (Perceptron(max_iter=1000, class_weight = 'balanced'), \"Perceptron\", None),\n",
    "        (PassiveAggressiveClassifier(max_iter=1000), \"Passive_Aggressive\", {'C':[0.01,0.1,1,10,100]}),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\",{'n_neighbors': np.linspace(1,100,10,dtype=int), 'p':[1,2]}),\n",
    "        (RandomForestClassifier(n_estimators=1000, class_weight = 'balanced', random_state = 99), \"Random_forest\", {'n_estimators':np.linspace(20,300,10,dtype=int),'max_depth':np.linspace(1,21,5,dtype=int)}),\n",
    "        (LinearSVC(penalty=\"l1\",dual=False, tol=1e-3, class_weight = 'balanced', max_iter=10000),\"LinearSVC_L1\", {'C':[0.01, 0.1, 1,10,100]}),\n",
    "        (LinearSVC(penalty=\"l2\",dual=False, tol=1e-3, class_weight = 'balanced'),\"LinearSVC_L2\", {'C':[0.01, 0.1, 1,10,100]}),\n",
    "        (SGDClassifier(alpha=.0001, max_iter=1000,penalty=\"l1\", class_weight = 'balanced', random_state = 99),\"SGDClassifier_L1\", None),\n",
    "        (SGDClassifier(alpha=.0001, max_iter=1000,penalty=\"l2\", class_weight = 'balanced', random_state = 99),\"SGDClassifier_L2\", None),\n",
    "        (SGDClassifier(alpha=.0001, max_iter=1000,penalty=\"elasticnet\", class_weight = 'balanced', random_state = 99),\"Elastic_Net_penalty\", None),\n",
    "        #(NearestCentroid(),\"NearestCentroid\" ),\n",
    "        (MultinomialNB(alpha=.01),\"MultinomialNB\", None),\n",
    "        (BernoulliNB(alpha=.01),\"BernoulliNB\", None),\n",
    "        (LogisticRegression(C=10, class_weight = 'balanced', max_iter=10000, random_state = 99), \"Logistic_Regression\", {'C':[0.01,0.1,1,10,100]}),\n",
    "        (SVC(C=1, class_weight = 'balanced', kernel = 'rbf', probability = True, random_state = 99), 'SVC_rbf', {'C':[0.01,0.1,1,10,100]}),\n",
    "        (SVC(C=1, class_weight = 'balanced', kernel = 'poly', probability = True, random_state = 99), 'SVC_poly', {'C':[0.01,0.1,1,10,100]}),\n",
    "        (SVC(C=1, class_weight = 'balanced', kernel = 'sigmoid', probability = True, random_state = 99), 'SVC_sigmoid', {'C':[0.01,0.1,1,10,100]})\n",
    "        ):\n",
    "    \n",
    "    best_param = find_best_param(clf, name, param_test)\n",
    "    best_params.append(best_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge_Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(alpha=1.0, class_weight='balanced', copy_X=True,\n",
      "                fit_intercept=True, max_iter=1000, normalize=False,\n",
      "                random_state=99, solver='sag', tol=0.01)\n",
      "\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron(alpha=0.0001, class_weight='balanced', early_stopping=False,\n",
      "           eta0=1.0, fit_intercept=True, max_iter=1000, n_iter_no_change=5,\n",
      "           n_jobs=None, penalty=None, random_state=99, shuffle=True, tol=0.001,\n",
      "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "================================================================================\n",
      "Passive_Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=0.01, average=False, class_weight=None,\n",
      "                            early_stopping=False, fit_intercept=True,\n",
      "                            loss='hinge', max_iter=1000, n_iter_no_change=5,\n",
      "                            n_jobs=None, random_state=99, shuffle=True,\n",
      "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "                            warm_start=False)\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "                     weights='uniform')\n",
      "\n",
      "================================================================================\n",
      "Random_forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=21, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=82,\n",
      "                       n_jobs=None, oob_score=False, random_state=99, verbose=0,\n",
      "                       warm_start=False)\n",
      "\n",
      "================================================================================\n",
      "LinearSVC_L1\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=10, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=10000,\n",
      "          multi_class='ovr', penalty='l1', random_state=99, tol=0.001,\n",
      "          verbose=0)\n",
      "\n",
      "================================================================================\n",
      "LinearSVC_L2\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=0.1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=99, tol=0.001,\n",
      "          verbose=0)\n",
      "\n",
      "================================================================================\n",
      "SGDClassifier_L1\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
      "              power_t=0.5, random_state=99, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "================================================================================\n",
      "SGDClassifier_L2\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=99, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "================================================================================\n",
      "Elastic_Net_penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
      "              penalty='elasticnet', power_t=0.5, random_state=99, shuffle=True,\n",
      "              tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "================================================================================\n",
      "MultinomialNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "================================================================================\n",
      "BernoulliNB\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "================================================================================\n",
      "Logistic_Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=99, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "\n",
      "================================================================================\n",
      "SVC_rbf\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=True, random_state=99, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "\n",
      "================================================================================\n",
      "SVC_poly\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
      "    max_iter=-1, probability=True, random_state=99, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "\n",
      "================================================================================\n",
      "SVC_sigmoid\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SVC(C=0.1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='sigmoid',\n",
      "    max_iter=-1, probability=True, random_state=99, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver='sag', max_iter = 1000, class_weight = 'balanced', random_state = 99), \"Ridge_Classifier\"),\n",
    "        (Perceptron(max_iter=1000, class_weight = 'balanced', random_state = 99), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(max_iter=1000, C = best_params[2]['C'], random_state = 99), \"Passive_Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors = best_params[3]['n_neighbors'], p = best_params[3]['p']), \"kNN\"),\n",
    "        (RandomForestClassifier(class_weight = 'balanced', max_depth = best_params[4]['max_depth'], n_estimators = best_params[4]['n_estimators'], random_state = 99), \"Random_forest\"),\n",
    "        (LinearSVC(max_iter=10000, penalty=\"l1\",dual=False, tol=1e-3, class_weight = 'balanced', C = best_params[5]['C'], random_state = 99),\"LinearSVC_L1\"),\n",
    "        (LinearSVC(penalty=\"l2\",dual=False, tol=1e-3, class_weight = 'balanced', C= best_params[6]['C'], random_state = 99),\"LinearSVC_L2\"),\n",
    "        (SGDClassifier(alpha=.0001, max_iter=1000,penalty=\"l1\", class_weight = 'balanced', random_state = 99),\"SGDClassifier_L1\"),\n",
    "        (SGDClassifier(alpha=.0001, max_iter=1000,penalty=\"l2\", class_weight = 'balanced', random_state = 99),\"SGDClassifier_L2\"),\n",
    "        (SGDClassifier(alpha=.0001, max_iter=1000,penalty=\"elasticnet\", class_weight = 'balanced', random_state = 99),\"Elastic_Net_penalty\"),\n",
    "        #(NearestCentroid(),\"NearestCentroid\" ),\n",
    "        (MultinomialNB(alpha=.01),\"MultinomialNB\"),\n",
    "        (BernoulliNB(alpha=.01),\"BernoulliNB\"),\n",
    "        (LogisticRegression(class_weight = 'balanced', C = best_params[12]['C'], max_iter = 1000, random_state = 99), \"Logistic_Regression\"),\n",
    "        (SVC(class_weight = 'balanced', kernel = 'rbf', C = best_params[13]['C'], probability = True, random_state = 99), 'SVC_rbf'),\n",
    "        (SVC(C=best_params[14]['C'], class_weight = 'balanced', kernel = 'poly', probability = True, random_state = 99), 'SVC_poly'),\n",
    "        (SVC(C=best_params[15]['C'], class_weight = 'balanced', kernel = 'sigmoid', probability = True, random_state = 99), 'SVC_sigmoid')\n",
    "):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf,name))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(12)]\n",
    "names, accuracy, aucscore, precision_binary, recall_binary, fmeasure_binary, f1_micro, f1_macro, sensitivity, specificity, youden_index, threshold = results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge_Classifier\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "RidgeClassifier(alpha=1.0, class_weight='balanced', copy_X=True,\n",
      "                fit_intercept=True, max_iter=1000, normalize=False,\n",
      "                random_state=99, solver='sag', tol=0.01)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8534    0.7831    0.8167       461\n",
      "           1     0.8267    0.8850    0.8548       539\n",
      "\n",
      "    accuracy                         0.8380      1000\n",
      "   macro avg     0.8401    0.8340    0.8358      1000\n",
      "weighted avg     0.8390    0.8380    0.8373      1000\n",
      "\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "Perceptron(alpha=0.0001, class_weight='balanced', early_stopping=False,\n",
      "           eta0=1.0, fit_intercept=True, max_iter=1000, n_iter_no_change=5,\n",
      "           n_jobs=None, penalty=None, random_state=99, shuffle=True, tol=0.001,\n",
      "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8441    0.7636    0.8018       461\n",
      "           1     0.8130    0.8794    0.8449       539\n",
      "\n",
      "    accuracy                         0.8260      1000\n",
      "   macro avg     0.8286    0.8215    0.8234      1000\n",
      "weighted avg     0.8274    0.8260    0.8251      1000\n",
      "\n",
      "================================================================================\n",
      "Passive_Aggressive\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "PassiveAggressiveClassifier(C=0.01, average=False, class_weight=None,\n",
      "                            early_stopping=False, fit_intercept=True,\n",
      "                            loss='hinge', max_iter=1000, n_iter_no_change=5,\n",
      "                            n_jobs=None, random_state=99, shuffle=True,\n",
      "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "                            warm_start=False)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8562    0.8134    0.8343       461\n",
      "           1     0.8470    0.8831    0.8647       539\n",
      "\n",
      "    accuracy                         0.8510      1000\n",
      "   macro avg     0.8516    0.8483    0.8495      1000\n",
      "weighted avg     0.8512    0.8510    0.8507      1000\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "                     weights='uniform')\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6484    0.7961    0.7147       461\n",
      "           1     0.7834    0.6308    0.6989       539\n",
      "\n",
      "    accuracy                         0.7070      1000\n",
      "   macro avg     0.7159    0.7134    0.7068      1000\n",
      "weighted avg     0.7212    0.7070    0.7062      1000\n",
      "\n",
      "================================================================================\n",
      "Random_forest\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=21, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=82,\n",
      "                       n_jobs=None, oob_score=False, random_state=99, verbose=0,\n",
      "                       warm_start=False)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8326    0.8200    0.8262       461\n",
      "           1     0.8480    0.8590    0.8535       539\n",
      "\n",
      "    accuracy                         0.8410      1000\n",
      "   macro avg     0.8403    0.8395    0.8398      1000\n",
      "weighted avg     0.8409    0.8410    0.8409      1000\n",
      "\n",
      "================================================================================\n",
      "LinearSVC_L1\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "LinearSVC(C=10, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=10000,\n",
      "          multi_class='ovr', penalty='l1', random_state=99, tol=0.001,\n",
      "          verbose=0)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8416    0.7722    0.8054       461\n",
      "           1     0.8180    0.8757    0.8459       539\n",
      "\n",
      "    accuracy                         0.8280      1000\n",
      "   macro avg     0.8298    0.8240    0.8257      1000\n",
      "weighted avg     0.8289    0.8280    0.8272      1000\n",
      "\n",
      "================================================================================\n",
      "LinearSVC_L2\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "LinearSVC(C=0.1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=99, tol=0.001,\n",
      "          verbose=0)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8515    0.7961    0.8229       461\n",
      "           1     0.8348    0.8813    0.8574       539\n",
      "\n",
      "    accuracy                         0.8420      1000\n",
      "   macro avg     0.8432    0.8387    0.8401      1000\n",
      "weighted avg     0.8425    0.8420    0.8415      1000\n",
      "\n",
      "================================================================================\n",
      "SGDClassifier_L1\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l1',\n",
      "              power_t=0.5, random_state=99, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8676    0.7961    0.8303       461\n",
      "           1     0.8371    0.8961    0.8656       539\n",
      "\n",
      "    accuracy                         0.8500      1000\n",
      "   macro avg     0.8524    0.8461    0.8480      1000\n",
      "weighted avg     0.8512    0.8500    0.8493      1000\n",
      "\n",
      "================================================================================\n",
      "SGDClassifier_L2\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=99, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8625    0.8026    0.8315       461\n",
      "           1     0.8406    0.8905    0.8649       539\n",
      "\n",
      "    accuracy                         0.8500      1000\n",
      "   macro avg     0.8516    0.8466    0.8482      1000\n",
      "weighted avg     0.8507    0.8500    0.8495      1000\n",
      "\n",
      "================================================================================\n",
      "Elastic_Net_penalty\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
      "              penalty='elasticnet', power_t=0.5, random_state=99, shuffle=True,\n",
      "              tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8659    0.7983    0.8307       461\n",
      "           1     0.8383    0.8942    0.8654       539\n",
      "\n",
      "    accuracy                         0.8500      1000\n",
      "   macro avg     0.8521    0.8463    0.8480      1000\n",
      "weighted avg     0.8510    0.8500    0.8494      1000\n",
      "\n",
      "================================================================================\n",
      "MultinomialNB\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7277    0.6204    0.6698       461\n",
      "           1     0.7117    0.8015    0.7539       539\n",
      "\n",
      "    accuracy                         0.7180      1000\n",
      "   macro avg     0.7197    0.7109    0.7119      1000\n",
      "weighted avg     0.7191    0.7180    0.7151      1000\n",
      "\n",
      "================================================================================\n",
      "BernoulliNB\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7863    0.8460    0.8150       461\n",
      "           1     0.8591    0.8033    0.8303       539\n",
      "\n",
      "    accuracy                         0.8230      1000\n",
      "   macro avg     0.8227    0.8247    0.8227      1000\n",
      "weighted avg     0.8255    0.8230    0.8233      1000\n",
      "\n",
      "================================================================================\n",
      "Logistic_Regression\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=99, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8352    0.8134    0.8242       461\n",
      "           1     0.8439    0.8627    0.8532       539\n",
      "\n",
      "    accuracy                         0.8400      1000\n",
      "   macro avg     0.8396    0.8381    0.8387      1000\n",
      "weighted avg     0.8399    0.8400    0.8398      1000\n",
      "\n",
      "================================================================================\n",
      "SVC_rbf\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=True, random_state=99, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8472    0.8178    0.8322       461\n",
      "           1     0.8486    0.8738    0.8611       539\n",
      "\n",
      "    accuracy                         0.8480      1000\n",
      "   macro avg     0.8479    0.8458    0.8466      1000\n",
      "weighted avg     0.8480    0.8480    0.8478      1000\n",
      "\n",
      "================================================================================\n",
      "SVC_poly\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
      "    max_iter=-1, probability=True, random_state=99, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7598    0.8026    0.7806       461\n",
      "           1     0.8226    0.7829    0.8023       539\n",
      "\n",
      "    accuracy                         0.7920      1000\n",
      "   macro avg     0.7912    0.7928    0.7914      1000\n",
      "weighted avg     0.7936    0.7920    0.7923      1000\n",
      "\n",
      "================================================================================\n",
      "SVC_sigmoid\n",
      "________________________________________________________________________________\n",
      "Test: \n",
      "SVC(C=0.1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='sigmoid',\n",
      "    max_iter=-1, probability=True, random_state=99, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8067    0.8330    0.8196       461\n",
      "           1     0.8531    0.8293    0.8410       539\n",
      "\n",
      "    accuracy                         0.8310      1000\n",
      "   macro avg     0.8299    0.8311    0.8303      1000\n",
      "weighted avg     0.8317    0.8310    0.8312      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results2 = []\n",
    "for clf, name, thre in (\n",
    "        (RidgeClassifier(tol=1e-2, solver='sag', max_iter = 1000, class_weight = 'balanced', random_state = 99), \"Ridge_Classifier\", threshold[0]),\n",
    "        (Perceptron(max_iter=1000, class_weight = 'balanced', random_state = 99), \"Perceptron\", threshold[1]),\n",
    "        (PassiveAggressiveClassifier(max_iter=1000, C = best_params[2]['C'], random_state = 99), \"Passive_Aggressive\", threshold[2]),\n",
    "        (KNeighborsClassifier(n_neighbors = best_params[3]['n_neighbors'], p = best_params[3]['p']), \"kNN\", threshold[3]),\n",
    "        (RandomForestClassifier(class_weight = 'balanced', max_depth = best_params[4]['max_depth'], n_estimators = best_params[4]['n_estimators'], random_state = 99), \"Random_forest\", threshold[4]),\n",
    "        (LinearSVC(penalty=\"l1\",dual=False, tol=1e-3, class_weight = 'balanced', C = best_params[5]['C'], max_iter = 10000, random_state = 99),\"LinearSVC_L1\", threshold[5]),\n",
    "        (LinearSVC(penalty=\"l2\",dual=False, tol=1e-3, class_weight = 'balanced', C= best_params[6]['C'], max_iter = 1000, random_state = 99),\"LinearSVC_L2\", threshold[6]),\n",
    "        (SGDClassifier(alpha=.0001, max_iter=1000,penalty=\"l1\", class_weight = 'balanced', random_state = 99),\"SGDClassifier_L1\", threshold[7]),\n",
    "        (SGDClassifier(alpha=.0001, max_iter=1000,penalty=\"l2\", class_weight = 'balanced', random_state = 99),\"SGDClassifier_L2\", threshold[8]),\n",
    "        (SGDClassifier(alpha=.0001, max_iter=1000,penalty=\"elasticnet\", class_weight = 'balanced', random_state = 99),\"Elastic_Net_penalty\", threshold[9]),\n",
    "        #(NearestCentroid(),\"NearestCentroid\" ),\n",
    "        (MultinomialNB(alpha=.01),\"MultinomialNB\", threshold[10]),\n",
    "        (BernoulliNB(alpha=.01),\"BernoulliNB\", threshold[11]),\n",
    "        (LogisticRegression(class_weight = 'balanced', C = best_params[12]['C'], max_iter=1000, random_state = 99), \"Logistic_Regression\", threshold[10]),\n",
    "        (SVC(class_weight = 'balanced', C = best_params[13]['C'], kernel = 'rbf',probability = True, random_state = 99), 'SVC_rbf', threshold[11]),\n",
    "        (SVC(C=best_params[14]['C'], class_weight = 'balanced', kernel = 'poly', probability = True, random_state = 99), 'SVC_poly', threshold[12]),\n",
    "        (SVC(C=best_params[15]['C'], class_weight = 'balanced', kernel = 'sigmoid', probability = True, random_state = 99), 'SVC_sigmoid', threshold[13])\n",
    "):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results2.append(heldout(clf,name,thre))  \n",
    "\n",
    "results2 = [[x[i] for x in results2] for i in range(11)]\n",
    "names2, accuracy2, aucscore2, precision_binary2, recall_binary2, fmeasure_binary2, f1_micro2, f1_macro2, sensitivity2, specificity2, youden_index2  = results2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf_names</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>aucscore</th>\n",
       "      <th>precision_binary</th>\n",
       "      <th>recall_binary</th>\n",
       "      <th>f1_binary</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>youden_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge_Classifier</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.898225</td>\n",
       "      <td>0.82669</td>\n",
       "      <td>0.884972</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.83579</td>\n",
       "      <td>0.884972</td>\n",
       "      <td>0.78308</td>\n",
       "      <td>0.668052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.889286</td>\n",
       "      <td>0.813036</td>\n",
       "      <td>0.879406</td>\n",
       "      <td>0.84492</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.823371</td>\n",
       "      <td>0.879406</td>\n",
       "      <td>0.763557</td>\n",
       "      <td>0.642964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Passive_Aggressive</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.886437</td>\n",
       "      <td>0.846975</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.864668</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.849464</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.813449</td>\n",
       "      <td>0.696566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kNN</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.713447</td>\n",
       "      <td>0.78341</td>\n",
       "      <td>0.630798</td>\n",
       "      <td>0.698869</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.706786</td>\n",
       "      <td>0.630798</td>\n",
       "      <td>0.796095</td>\n",
       "      <td>0.426893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random_forest</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.903799</td>\n",
       "      <td>0.847985</td>\n",
       "      <td>0.858998</td>\n",
       "      <td>0.853456</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.839843</td>\n",
       "      <td>0.858998</td>\n",
       "      <td>0.819957</td>\n",
       "      <td>0.678955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC_L1</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.900861</td>\n",
       "      <td>0.818024</td>\n",
       "      <td>0.875696</td>\n",
       "      <td>0.845878</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.825654</td>\n",
       "      <td>0.875696</td>\n",
       "      <td>0.772234</td>\n",
       "      <td>0.64793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LinearSVC_L2</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.89443</td>\n",
       "      <td>0.834798</td>\n",
       "      <td>0.881262</td>\n",
       "      <td>0.857401</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.840135</td>\n",
       "      <td>0.881262</td>\n",
       "      <td>0.796095</td>\n",
       "      <td>0.677357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SGDClassifier_L1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.894015</td>\n",
       "      <td>0.837088</td>\n",
       "      <td>0.896104</td>\n",
       "      <td>0.865591</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.847954</td>\n",
       "      <td>0.896104</td>\n",
       "      <td>0.796095</td>\n",
       "      <td>0.692199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SGDClassifier_L2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.894603</td>\n",
       "      <td>0.84063</td>\n",
       "      <td>0.890538</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.848163</td>\n",
       "      <td>0.890538</td>\n",
       "      <td>0.802603</td>\n",
       "      <td>0.693141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elastic_Net_penalty</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.893625</td>\n",
       "      <td>0.838261</td>\n",
       "      <td>0.894249</td>\n",
       "      <td>0.86535</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.848025</td>\n",
       "      <td>0.894249</td>\n",
       "      <td>0.798265</td>\n",
       "      <td>0.692513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.819397</td>\n",
       "      <td>0.711697</td>\n",
       "      <td>0.801484</td>\n",
       "      <td>0.753927</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.711858</td>\n",
       "      <td>0.801484</td>\n",
       "      <td>0.62039</td>\n",
       "      <td>0.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.895321</td>\n",
       "      <td>0.859127</td>\n",
       "      <td>0.80334</td>\n",
       "      <td>0.830297</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.822672</td>\n",
       "      <td>0.80334</td>\n",
       "      <td>0.845987</td>\n",
       "      <td>0.649327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.895524</td>\n",
       "      <td>0.84392</td>\n",
       "      <td>0.862709</td>\n",
       "      <td>0.853211</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.838693</td>\n",
       "      <td>0.862709</td>\n",
       "      <td>0.813449</td>\n",
       "      <td>0.676158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVC_rbf</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.89111</td>\n",
       "      <td>0.848649</td>\n",
       "      <td>0.87384</td>\n",
       "      <td>0.86106</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.846645</td>\n",
       "      <td>0.87384</td>\n",
       "      <td>0.817787</td>\n",
       "      <td>0.691628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVC_poly</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.860276</td>\n",
       "      <td>0.822612</td>\n",
       "      <td>0.782931</td>\n",
       "      <td>0.802281</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.791436</td>\n",
       "      <td>0.782931</td>\n",
       "      <td>0.802603</td>\n",
       "      <td>0.585534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SVC_sigmoid</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.8751</td>\n",
       "      <td>0.853053</td>\n",
       "      <td>0.829314</td>\n",
       "      <td>0.841016</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.830327</td>\n",
       "      <td>0.829314</td>\n",
       "      <td>0.832972</td>\n",
       "      <td>0.662285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              clf_names accuracy  aucscore precision_binary recall_binary  \\\n",
       "0      Ridge_Classifier    0.838  0.898225          0.82669      0.884972   \n",
       "1            Perceptron    0.826  0.889286         0.813036      0.879406   \n",
       "2    Passive_Aggressive    0.851  0.886437         0.846975      0.883117   \n",
       "3                   kNN    0.707  0.713447          0.78341      0.630798   \n",
       "4         Random_forest    0.841  0.903799         0.847985      0.858998   \n",
       "5          LinearSVC_L1    0.828  0.900861         0.818024      0.875696   \n",
       "6          LinearSVC_L2    0.842   0.89443         0.834798      0.881262   \n",
       "7      SGDClassifier_L1     0.85  0.894015         0.837088      0.896104   \n",
       "8      SGDClassifier_L2     0.85  0.894603          0.84063      0.890538   \n",
       "9   Elastic_Net_penalty     0.85  0.893625         0.838261      0.894249   \n",
       "10        MultinomialNB    0.718  0.819397         0.711697      0.801484   \n",
       "11          BernoulliNB    0.823  0.895321         0.859127       0.80334   \n",
       "12  Logistic_Regression     0.84  0.895524          0.84392      0.862709   \n",
       "13              SVC_rbf    0.848   0.89111         0.848649       0.87384   \n",
       "14             SVC_poly    0.792  0.860276         0.822612      0.782931   \n",
       "15          SVC_sigmoid    0.831    0.8751         0.853053      0.829314   \n",
       "\n",
       "   f1_binary f1_micro  f1_macro sensitivity specificity youden_index  \n",
       "0   0.854839    0.838   0.83579    0.884972     0.78308     0.668052  \n",
       "1    0.84492    0.826  0.823371    0.879406    0.763557     0.642964  \n",
       "2   0.864668    0.851  0.849464    0.883117    0.813449     0.696566  \n",
       "3   0.698869    0.707  0.706786    0.630798    0.796095     0.426893  \n",
       "4   0.853456    0.841  0.839843    0.858998    0.819957     0.678955  \n",
       "5   0.845878    0.828  0.825654    0.875696    0.772234      0.64793  \n",
       "6   0.857401    0.842  0.840135    0.881262    0.796095     0.677357  \n",
       "7   0.865591     0.85  0.847954    0.896104    0.796095     0.692199  \n",
       "8   0.864865     0.85  0.848163    0.890538    0.802603     0.693141  \n",
       "9    0.86535     0.85  0.848025    0.894249    0.798265     0.692513  \n",
       "10  0.753927    0.718  0.711858    0.801484     0.62039     0.421875  \n",
       "11  0.830297    0.823  0.822672     0.80334    0.845987     0.649327  \n",
       "12  0.853211     0.84  0.838693    0.862709    0.813449     0.676158  \n",
       "13   0.86106    0.848  0.846645     0.87384    0.817787     0.691628  \n",
       "14  0.802281    0.792  0.791436    0.782931    0.802603     0.585534  \n",
       "15  0.841016    0.831  0.830327    0.829314    0.832972     0.662285  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW_distant_test_gridSearch = [names2,\n",
    "                      pd.DataFrame(accuracy2).mean(axis = 1), \n",
    "                      pd.DataFrame(aucscore2).mean(axis = 1), \n",
    "                      pd.DataFrame(precision_binary2).mean(axis = 1),\n",
    "                      pd.DataFrame(recall_binary2).mean(axis = 1),\n",
    "                      pd.DataFrame(fmeasure_binary2).mean(axis = 1),\n",
    "                      pd.DataFrame(f1_micro2).mean(axis = 1),\n",
    "                      pd.DataFrame(f1_macro2).mean(axis = 1),\n",
    "                      pd.DataFrame(sensitivity2).mean(axis = 1),\n",
    "                      pd.DataFrame(specificity2).mean(axis = 1),\n",
    "                      pd.DataFrame(youden_index2).mean(axis = 1)]\n",
    "BoW_distant_test_gridSearch = pd.DataFrame(BoW_distant_test_gridSearch).T\n",
    "BoW_distant_test_gridSearch.columns = ['clf_names', 'accuracy','aucscore','precision_binary', 'recall_binary','f1_binary','f1_micro', 'f1_macro', 'sensitivity', 'specificity', 'youden_index']\n",
    "\n",
    "BoW_distant_test_gridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for the best classifier (Random Forest in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params[4]['max_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params[4]['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_nlp",
   "language": "python",
   "name": "hf_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
